action: "train_and_save" # train or test
name: ??? # name of the experiment needed for the logs
modality: ["EMG"] # modality used

total_batch: 128 # total batch size if training is done with gradient accumulation
batch_size: 32 # batch size for the forward

gpus: null # gpus adopted
wandb_name: "EMG-VAE" # needed for wandb logging
resume_from: null # checkpoint directory
logname: null # name of the logs
models_dir: null # directory containing all the models
split: null

train:
  num_iter: 300 # number of training iterations with total_batch size
  lr_steps: 50 # steps before reducing learning rate
  eval_freq: 50 # evaluation frequency
  num_clips: 5 # clips adopted in training
  EMG:
    feature_size: 1664
  bottleneck_size: 256

save: 
  num_clips: 5 # clips adopted in save

test:
  num_clips: 5 # number of clips in testing


dataset:
  annotations_path: "../an_data/EMG_preprocessed/"
  shift: D1-D1
  workers: 4
  stride: 2
  resolution: 224
  num_classes: 20
  EMG:
    data_path: "../an_data/EMG_saved_features/"
  RGB:
    data_path: null
    
models:
  EMG:
    architecture:
      bottleneck: 256
    model: VariationalAutoencoder
    dropout: 0.2
    kwargs: {}
    epochs: 100
    lr_steps: 30 
    lr: 0.001
    lr_gamma: 0.01
    beta: 0.00001